{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6b90c4-3582-43d0-9310-f1c6850b62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53243ade-1a78-49ee-abca-a74ccdfaef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected project root: C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\n",
      "Looking for .env at: C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\\.env\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "candidates = [cwd, cwd.parent, cwd.parent.parent]\n",
    "\n",
    "project_root = None\n",
    "for c in candidates:\n",
    "    if (c / \"data\").exists() and (c / \"src\").exists():\n",
    "        project_root = c\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    project_root = cwd.parent\n",
    "\n",
    "env_path = project_root / \".env\"\n",
    "\n",
    "print(\"Detected project root:\", project_root)\n",
    "print(\"Looking for .env at:\", env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b819e34-2c35-4e43-aee4-155dc27fb45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\n",
      "Data directory: C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\\data\n",
      "Output directory for processed chunks: C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = project_root / \"data\"\n",
    "OUTPUT_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory for processed chunks: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09bbe06e-e962-4f95-ae05-96892fb5ca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following files for ingestion:\n",
      " - data\\sample_data.pdf\n"
     ]
    }
   ],
   "source": [
    "supported_extensions = {\".pdf\", \".txt\", \".md\"}\n",
    "all_files = [\n",
    "    p for p in DATA_DIR.rglob(\"*\")\n",
    "    if p.is_file() and p.suffix.lower() in supported_extensions\n",
    "]\n",
    "\n",
    "if not all_files:\n",
    "    print(\"No PDF/TXT/MD files found in data/. \"\n",
    "          \"Add at least one lecture file (e.g., sample_lecture.pdf) and rerun this cell.\")\n",
    "else:\n",
    "    print(\"Found the following files for ingestion:\")\n",
    "    for p in all_files:\n",
    "        print(\" -\", p.relative_to(project_root))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b01b504e-3e08-4dd1-ad22-4b5650d77778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_file(path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a single file (PDF, TXT, or MD) into a list of LangChain Document objects.\n",
    "    Each page (for PDF) or whole file (for text) becomes a Document.\n",
    "    \"\"\"\n",
    "    suffix = path.suffix.lower()\n",
    "\n",
    "    if suffix == \".pdf\":\n",
    "        loader = PyPDFLoader(str(path))\n",
    "        docs = loader.load()\n",
    "    elif suffix in {\".txt\", \".md\"}:\n",
    "        loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {suffix}\")\n",
    "    course = \"default_course\"\n",
    "    lecture_id = path.stem  \n",
    "\n",
    "    for d in docs:\n",
    "        d.metadata.setdefault(\"course\", course)\n",
    "        d.metadata.setdefault(\"lecture_id\", lecture_id)\n",
    "        d.metadata.setdefault(\"source\", str(path.relative_to(project_root)))\n",
    "\n",
    "        d.metadata.setdefault(\"page\", d.metadata.get(\"page\", 0))\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b43e76a7-b343-4e6a-b1cd-0fa49c9d4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_files(paths: List[Path]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all given files into a single list of Documents.\n",
    "    Errors on one file are caught and reported without stopping the whole process.\n",
    "    \"\"\"\n",
    "    all_docs: List[Document] = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            docs = load_single_file(path)\n",
    "            all_docs.extend(docs)\n",
    "            print(f\"Loaded {len(docs)} document(s) from {path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading {path.name}: {e}\")\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd483958-fe67-4bd8-a1fa-8cc90f8fb653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 document(s) from sample_data.pdf\n",
      "\n",
      "Total loaded documents (pages/segments): 25\n",
      "\n",
      "Example document metadata and first 400 characters of content:\n",
      "\n",
      "Metadata: {'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251125214124', 'source': 'C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\Capstone-MAT496\\\\data\\\\sample_data.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'course': 'default_course', 'lecture_id': 'sample_data'}\n",
      "\n",
      "Content preview:\n",
      " 8\n",
      "Modelling Long-Run Relationships in Finance\n",
      "LEARNING OUTCOMES\n",
      "In this chapter, you will learn how to\n",
      "Highlight the problems that may occur if non-stationary data are\n",
      "used in their levels form\n",
      "Test for unit roots\n",
      "Examine whether systems of variables are cointegrated\n",
      "Estimate error correction and vector error correction models\n",
      "Explain the intuition behind Johansen’s test for cointegration\n",
      "Describe\n"
     ]
    }
   ],
   "source": [
    "if not all_files:\n",
    "    raw_docs: List[Document] = []\n",
    "    print(\"No files to load yet. Once you add files into data/, rerun this cell.\")\n",
    "else:\n",
    "    raw_docs = load_all_files(all_files)\n",
    "    print(f\"\\nTotal loaded documents (pages/segments): {len(raw_docs)}\")\n",
    "\n",
    "if raw_docs:\n",
    "    print(\"\\nExample document metadata and first 400 characters of content:\\n\")\n",
    "    example_doc = raw_docs[0]\n",
    "    print(\"Metadata:\", example_doc.metadata)\n",
    "    print(\"\\nContent preview:\\n\", example_doc.page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a756b8be-7fb7-4f03-9d0e-9b4ff4ed0de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text splitter configured.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,       \n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "print(\"Text splitter configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d0b3613-7bc2-4a08-86b9-313af771364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks while preserving key metadata.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Add a simple chunk_id inside metadata for traceability\n",
    "    for idx, c in enumerate(chunks):\n",
    "        c.metadata.setdefault(\"chunk_id\", idx)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e840d4e-ef28-42ba-8088-bcbb6473dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 75\n",
      "\n",
      "Example chunk metadata and preview:\n",
      "\n",
      "Metadata: {'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251125214124', 'source': 'C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\Capstone-MAT496\\\\data\\\\sample_data.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'course': 'default_course', 'lecture_id': 'sample_data', 'chunk_id': 0}\n",
      "\n",
      "Chunk content preview:\n",
      " 8\n",
      "Modelling Long-Run Relationships in Finance\n",
      "LEARNING OUTCOMES\n",
      "In this chapter, you will learn how to\n",
      "Highlight the problems that may occur if non-stationary data are\n",
      "used in their levels form\n",
      "Test for unit roots\n",
      "Examine whether systems of variables are cointegrated\n",
      "Estimate error correction and vector error correction models\n",
      "Explain the intuition behind Johansen’s test for cointegration\n",
      "Describe\n"
     ]
    }
   ],
   "source": [
    "chunked_docs = chunk_documents(raw_docs)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunked_docs)}\")\n",
    "if chunked_docs:\n",
    "    print(\"\\nExample chunk metadata and preview:\\n\")\n",
    "    example_chunk = chunked_docs[0]\n",
    "    print(\"Metadata:\", example_chunk.metadata)\n",
    "    print(\"\\nChunk content preview:\\n\", example_chunk.page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "384e83e0-4fce-46ed-b54b-b9bce75f9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_dataframe(chunks: List[Document]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a list of Document chunks into a pandas DataFrame\n",
    "    with separate columns for content and metadata.\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for c in chunks:\n",
    "        meta = c.metadata.copy()\n",
    "        record = {\n",
    "            \"text\": c.page_content,\n",
    "            \"course\": meta.get(\"course\", \"default_course\"),\n",
    "            \"lecture_id\": meta.get(\"lecture_id\", \"\"),\n",
    "            \"source\": meta.get(\"source\", \"\"),\n",
    "            \"page\": meta.get(\"page\", None),\n",
    "            \"chunk_id\": meta.get(\"chunk_id\", None),\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5808816-4f61-4e37-8115-59ecea0e37b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (75, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>course</th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8\\nModelling Long-Run Relationships in Finance...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and why it is essential that variables that ar...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>behaviour and properties\\n. To offer one illus...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t\\n will not have a\\nsmaller effect in time \\n...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are totally unrelated. So, if standard regress...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          course  \\\n",
       "0  8\\nModelling Long-Run Relationships in Finance...  default_course   \n",
       "1  and why it is essential that variables that ar...  default_course   \n",
       "2  behaviour and properties\\n. To offer one illus...  default_course   \n",
       "3  t\\n will not have a\\nsmaller effect in time \\n...  default_course   \n",
       "4  are totally unrelated. So, if standard regress...  default_course   \n",
       "\n",
       "    lecture_id                                             source  page  \\\n",
       "0  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     0   \n",
       "1  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     0   \n",
       "2  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     1   \n",
       "3  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     1   \n",
       "4  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     1   \n",
       "\n",
       "   chunk_id  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  \n",
       "3         3  \n",
       "4         4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved chunks to:\n",
      "- C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\\data\\processed\\lecture_chunks.csv\n",
      "- C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\\data\\processed\\lecture_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "if not chunked_docs:\n",
    "    print(\"No chunks to save yet. Add files to data/ and rerun the earlier cells.\")\n",
    "else:\n",
    "    chunks_df = chunks_to_dataframe(chunked_docs)\n",
    "    print(\"DataFrame shape:\", chunks_df.shape)\n",
    "    display(chunks_df.head(5))\n",
    "\n",
    "    # Save to CSV and Parquet for later notebooks\n",
    "    csv_path = OUTPUT_DIR / \"lecture_chunks.csv\"\n",
    "    parquet_path = OUTPUT_DIR / \"lecture_chunks.parquet\"\n",
    "\n",
    "    chunks_df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    chunks_df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    print(f\"\\nSaved chunks to:\\n- {csv_path}\\n- {parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb7dcb65-8ace-4926-b197-536ac3bf6ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic stats:\n",
      " - Unique lectures: 1\n",
      " - Total chunks: 75\n",
      "\n",
      "Chunks per lecture_id:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lecture_id\n",
       "sample_data    75\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if chunked_docs:\n",
    "    print(\"Basic stats:\")\n",
    "    print(\" - Unique lectures:\", chunks_df[\"lecture_id\"].nunique())\n",
    "    print(\" - Total chunks:\", len(chunks_df))\n",
    "    print(\"\\nChunks per lecture_id:\")\n",
    "    display(chunks_df[\"lecture_id\"].value_counts())\n",
    "else:\n",
    "    print(\"No chunk statistics to show yet.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
