{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e6e904-c237-4da9-acb4-092f0b98c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65b6b6a-b661-42cd-a4cd-f846c9643629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from operator import add\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3478787e-05c7-4278-9201-f3a856dac576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected project root: C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\n",
      "Loaded chunks from C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\\data\\processed\\lecture_chunks.parquet\n",
      "Chunks DataFrame shape: (75, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>course</th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8\\nModelling Long-Run Relationships in Finance...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and why it is essential that variables that ar...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>behaviour and properties\\n. To offer one illus...</td>\n",
       "      <td>default_course</td>\n",
       "      <td>sample_data</td>\n",
       "      <td>C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          course  \\\n",
       "0  8\\nModelling Long-Run Relationships in Finance...  default_course   \n",
       "1  and why it is essential that variables that ar...  default_course   \n",
       "2  behaviour and properties\\n. To offer one illus...  default_course   \n",
       "\n",
       "    lecture_id                                             source  page  \\\n",
       "0  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     0   \n",
       "1  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     0   \n",
       "2  sample_data  C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT49...     1   \n",
       "\n",
       "   chunk_id  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cwd = Path.cwd()\n",
    "candidates = [cwd, cwd.parent, cwd.parent.parent]\n",
    "\n",
    "project_root = None\n",
    "for c in candidates:\n",
    "    if (c / \"data\").exists() and (c / \"src\").exists():\n",
    "        project_root = c\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    project_root = cwd.parent\n",
    "\n",
    "print(\"Detected project root:\", project_root)\n",
    "\n",
    "env_path = project_root / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "else:\n",
    "    print(\"NOTE: .env loaded if present; not required in this notebook.\")\n",
    "\n",
    "DATA_DIR = project_root / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "chunks_parquet = PROCESSED_DIR / \"lecture_chunks.parquet\"\n",
    "chunks_csv = PROCESSED_DIR / \"lecture_chunks.csv\"\n",
    "\n",
    "if chunks_parquet.exists():\n",
    "    chunks_df = pd.read_parquet(chunks_parquet)\n",
    "    print(f\"Loaded chunks from {chunks_parquet}\")\n",
    "elif chunks_csv.exists():\n",
    "    chunks_df = pd.read_csv(chunks_csv)\n",
    "    print(f\"Loaded chunks from {chunks_csv}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find lecture_chunks.parquet or lecture_chunks.csv in data/processed/. \"\n",
    "        \"Run Notebook 1 first.\"\n",
    "    )\n",
    "\n",
    "print(\"Chunks DataFrame shape:\", chunks_df.shape)\n",
    "display(chunks_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a1a79-19e7-4d8a-8a10-e78f50ed6974",
   "metadata": {},
   "source": [
    "# REUSING SCHEMAS FROM PREVIOUS NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69eeefc-587e-4710-beb9-aae9ef9b648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCQQuestion(BaseModel):\n",
    "    id: str\n",
    "    stem: str\n",
    "    options: List[str]\n",
    "    correct_option_index: int\n",
    "    difficulty: str\n",
    "    topic: str\n",
    "    source_excerpt: str\n",
    "    source_doc_id: str\n",
    "\n",
    "\n",
    "class ShortAnswerQuestion(BaseModel):\n",
    "    id: str\n",
    "    question: str\n",
    "    ideal_answer: str\n",
    "    difficulty: str\n",
    "    topic: str\n",
    "    source_excerpt: str\n",
    "    source_doc_id: str\n",
    "\n",
    "\n",
    "class ConceptMapEdge(BaseModel):\n",
    "    id: str\n",
    "    source_concept: str\n",
    "    target_concept: str\n",
    "    relation: str\n",
    "    topic: str\n",
    "    source_excerpt: str\n",
    "    source_doc_id: str\n",
    "\n",
    "\n",
    "class QuestionBundle(BaseModel):\n",
    "    mcqs: List[MCQQuestion] = []\n",
    "    short_answers: List[ShortAnswerQuestion] = []\n",
    "    concept_edges: List[ConceptMapEdge] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f03854-652b-4dec-8361-0330da953561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (75, 5000)\n"
     ]
    }
   ],
   "source": [
    "corpus = chunks_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d2452ce-6ab3-48a0-a2a5-ce432c151a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_local(query: str, k: int = 4):\n",
    "    if not isinstance(query, str) or not query.strip():\n",
    "        raise ValueError(\"Query must be a non-empty string.\")\n",
    "\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    sims = cosine_similarity(query_vec, tfidf_matrix)[0]\n",
    "\n",
    "    k = min(k, len(sims))\n",
    "    top_indices = np.argsort(sims)[-k:][::-1]\n",
    "\n",
    "    rows = []\n",
    "    for rank, idx in enumerate(top_indices, start=1):\n",
    "        row = chunks_df.iloc[idx]\n",
    "        rows.append({\n",
    "            \"rank\": rank,\n",
    "            \"similarity\": float(sims[idx]),\n",
    "            \"lecture_id\": row.get(\"lecture_id\", \"\"),\n",
    "            \"page\": row.get(\"page\", None),\n",
    "            \"source\": row.get(\"source\", \"\"),\n",
    "            \"chunk_id\": row.get(\"chunk_id\", None),\n",
    "            \"text\": str(row[\"text\"]),\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(rows)\n",
    "    return top_indices, result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ddf9d72-6fd8-4219-a605-45526d9a3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_source_doc_id(row) -> str:\n",
    "    lecture_id = str(row.get(\"lecture_id\", \"\"))\n",
    "    page = row.get(\"page\", None)\n",
    "    chunk_id = row.get(\"chunk_id\", None)\n",
    "    return f\"{lecture_id}:page={page}:chunk={chunk_id}\"\n",
    "\n",
    "\n",
    "def extract_excerpt(text: str, max_chars: int = 300) -> str:\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    return text[: max_chars - 3] + \"...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75255405-a95b-4dbb-8141-8786b1206c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mcq_prompt_from_df(topic: str, ctx_df: pd.DataFrame) -> str:\n",
    "    context_blocks = []\n",
    "    for _, row in ctx_df.iterrows():\n",
    "        source_id = build_source_doc_id(row)\n",
    "        excerpt = extract_excerpt(row[\"text\"])\n",
    "        block = f\"[SOURCE_ID: {source_id}]\\n{excerpt}\"\n",
    "        context_blocks.append(block)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI tutor helping a teacher create exam-style multiple-choice questions.\n",
    "\n",
    "Use ONLY the information in the CONTEXT below.\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "TASK:\n",
    "Generate 3 clear, unambiguous multiple-choice questions about the topic: \"{topic}\".\n",
    "\n",
    "Rules:\n",
    "- Each question has exactly 4 options and exactly one correct option.\n",
    "- Options are mutually exclusive.\n",
    "- For each question, include id, stem, options, correct_option_index, difficulty, topic, source_excerpt, source_doc_id.\n",
    "\n",
    "Return ONLY valid JSON matching this structure:\n",
    "\n",
    "{{\n",
    "  \"mcqs\": [...],\n",
    "  \"short_answers\": [],\n",
    "  \"concept_edges\": []\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33338ab0-447c-4e4b-8d4e-080f4fb83d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGenState(TypedDict):\n",
    "    topic: str\n",
    "    mode: str                         \n",
    "    n_target: int                      \n",
    "    retrieved_context: List[Dict[str, Any]]\n",
    "    last_prompt: str\n",
    "    last_raw_output: str\n",
    "    questions_mcq: Annotated[List[Dict[str, Any]], add]\n",
    "    errors: Annotated[List[str], add]\n",
    "    done: bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19660e26-c166-4f13-9af7-4b9bd022dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: QGenState) -> QGenState:\n",
    "    topic = state[\"topic\"]\n",
    "    _, ctx_df = semantic_search_local(topic, k=4)\n",
    "\n",
    "    state[\"retrieved_context\"] = ctx_df.to_dict(orient=\"records\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b979261-c0cf-47ae-b75f-7c58b5b1dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_node(state: QGenState) -> QGenState:\n",
    "    topic = state[\"topic\"]\n",
    "    mode = state[\"mode\"]\n",
    "\n",
    "    ctx_df = pd.DataFrame(state[\"retrieved_context\"])\n",
    "    if ctx_df.empty:\n",
    "        state[\"errors\"].append(\"No context retrieved for topic.\")\n",
    "        state[\"last_prompt\"] = \"\"\n",
    "        return state\n",
    "\n",
    "    if mode == \"mcq\":\n",
    "        prompt = build_mcq_prompt_from_df(topic, ctx_df)\n",
    "    else:\n",
    "        prompt = f\"(Mode {mode} not yet implemented.)\"\n",
    "\n",
    "    state[\"last_prompt\"] = prompt\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee47078-9683-4e87-bbab-8a2f08deb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_llm_node(state: QGenState) -> QGenState:\n",
    "    \"\"\"\n",
    "    Placeholder 'LLM' that returns a fixed, valid JSON bundle.\n",
    "    Replace later with a real Perplexity/LLM call + structured output.\n",
    "    \"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    fake_json = {\n",
    "        \"mcqs\": [\n",
    "            {\n",
    "                \"id\": f\"{topic}_mcq_1\",\n",
    "                \"stem\": f\"Dummy question about {topic} (1)\",\n",
    "                \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "                \"correct_option_index\": 0,\n",
    "                \"difficulty\": \"easy\",\n",
    "                \"topic\": topic,\n",
    "                \"source_excerpt\": \"Dummy excerpt 1.\",\n",
    "                \"source_doc_id\": \"dummy:page=0:chunk=0\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": f\"{topic}_mcq_2\",\n",
    "                \"stem\": f\"Dummy question about {topic} (2)\",\n",
    "                \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "                \"correct_option_index\": 1,\n",
    "                \"difficulty\": \"medium\",\n",
    "                \"topic\": topic,\n",
    "                \"source_excerpt\": \"Dummy excerpt 2.\",\n",
    "                \"source_doc_id\": \"dummy:page=0:chunk=1\"\n",
    "            },\n",
    "        ],\n",
    "        \"short_answers\": [],\n",
    "        \"concept_edges\": []\n",
    "    }\n",
    "\n",
    "    state[\"last_raw_output\"] = json.dumps(fake_json)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e4ded9-8232-47f3-87e7-4b376cc15a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_node(state: QGenState) -> QGenState:\n",
    "    raw = state.get(\"last_raw_output\", \"\")\n",
    "    if not raw:\n",
    "        state[\"errors\"].append(\"No raw output to validate.\")\n",
    "        return state\n",
    "\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except json.JSONDecodeError as e:\n",
    "        state[\"errors\"].append(f\"JSON decode error: {e}\")\n",
    "        return state\n",
    "\n",
    "    try:\n",
    "        bundle = QuestionBundle(**data)\n",
    "    except ValidationError as e:\n",
    "        state[\"errors\"].append(f\"Pydantic validation error: {e}\")\n",
    "        return state\n",
    "\n",
    "    for mcq in bundle.mcqs:\n",
    "        state[\"questions_mcq\"].append(mcq.model_dump())\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce751499-33b6-4d6e-88d7-6e293a795274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_next_node(state: QGenState) -> str:\n",
    "    \"\"\"\n",
    "    Decide whether we have enough questions.\n",
    "    Return 'retrieve' to loop or END to finish.\n",
    "    \"\"\"\n",
    "    n_target = state[\"n_target\"]\n",
    "    current = len(state.get(\"questions_mcq\", []))\n",
    "\n",
    "    if current >= n_target:\n",
    "        state[\"done\"] = True\n",
    "        return END\n",
    "\n",
    "    if current > 0:\n",
    "        state[\"done\"] = True\n",
    "        return END\n",
    "\n",
    "    return \"retrieve\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d53fb64-7dbb-45fc-a1b1-f6fcf96abffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph question-generation app compiled.\n"
     ]
    }
   ],
   "source": [
    "graph_builder = StateGraph(QGenState)\n",
    "\n",
    "graph_builder.add_node(\"retrieve\", retrieve_node)\n",
    "graph_builder.add_node(\"build_prompt\", build_prompt_node)\n",
    "graph_builder.add_node(\"llm_call\", dummy_llm_node)\n",
    "graph_builder.add_node(\"validate\", validate_node)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"build_prompt\")\n",
    "graph_builder.add_edge(\"build_prompt\", \"llm_call\")\n",
    "graph_builder.add_edge(\"llm_call\", \"validate\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"validate\",\n",
    "    decide_next_node,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "graph_builder.set_entry_point(\"retrieve\")\n",
    "\n",
    "qgen_app = graph_builder.compile()\n",
    "print(\"LangGraph question-generation app compiled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c31009b-e882-45e4-90f9-55f97f65ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag: False\n",
      "Errors: []\n",
      "Number of MCQs generated: 4\n"
     ]
    }
   ],
   "source": [
    "initial_state: QGenState = {\n",
    "    \"topic\": \"probability distribution\",  # adjust to your notes\n",
    "    \"mode\": \"mcq\",\n",
    "    \"n_target\": 3,\n",
    "    \"retrieved_context\": [],\n",
    "    \"last_prompt\": \"\",\n",
    "    \"last_raw_output\": \"\",\n",
    "    \"questions_mcq\": [],\n",
    "    \"errors\": [],\n",
    "    \"done\": False,\n",
    "}\n",
    "\n",
    "result_state = qgen_app.invoke(initial_state)\n",
    "\n",
    "print(\"Done flag:\", result_state[\"done\"])\n",
    "print(\"Errors:\", result_state[\"errors\"])\n",
    "print(\"Number of MCQs generated:\", len(result_state[\"questions_mcq\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "082351ec-e97b-47f0-b66c-fdc4aa5696d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stem</th>\n",
       "      <th>options</th>\n",
       "      <th>correct_option_index</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>topic</th>\n",
       "      <th>source_excerpt</th>\n",
       "      <th>source_doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>probability distribution_mcq_1</td>\n",
       "      <td>Dummy question about probability distribution (1)</td>\n",
       "      <td>[Option A, Option B, Option C, Option D]</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>probability distribution</td>\n",
       "      <td>Dummy excerpt 1.</td>\n",
       "      <td>dummy:page=0:chunk=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probability distribution_mcq_2</td>\n",
       "      <td>Dummy question about probability distribution (2)</td>\n",
       "      <td>[Option A, Option B, Option C, Option D]</td>\n",
       "      <td>1</td>\n",
       "      <td>medium</td>\n",
       "      <td>probability distribution</td>\n",
       "      <td>Dummy excerpt 2.</td>\n",
       "      <td>dummy:page=0:chunk=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>probability distribution_mcq_1</td>\n",
       "      <td>Dummy question about probability distribution (1)</td>\n",
       "      <td>[Option A, Option B, Option C, Option D]</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>probability distribution</td>\n",
       "      <td>Dummy excerpt 1.</td>\n",
       "      <td>dummy:page=0:chunk=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probability distribution_mcq_2</td>\n",
       "      <td>Dummy question about probability distribution (2)</td>\n",
       "      <td>[Option A, Option B, Option C, Option D]</td>\n",
       "      <td>1</td>\n",
       "      <td>medium</td>\n",
       "      <td>probability distribution</td>\n",
       "      <td>Dummy excerpt 2.</td>\n",
       "      <td>dummy:page=0:chunk=1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0  probability distribution_mcq_1   \n",
       "1  probability distribution_mcq_2   \n",
       "2  probability distribution_mcq_1   \n",
       "3  probability distribution_mcq_2   \n",
       "\n",
       "                                                stem  \\\n",
       "0  Dummy question about probability distribution (1)   \n",
       "1  Dummy question about probability distribution (2)   \n",
       "2  Dummy question about probability distribution (1)   \n",
       "3  Dummy question about probability distribution (2)   \n",
       "\n",
       "                                    options  correct_option_index difficulty  \\\n",
       "0  [Option A, Option B, Option C, Option D]                     0       easy   \n",
       "1  [Option A, Option B, Option C, Option D]                     1     medium   \n",
       "2  [Option A, Option B, Option C, Option D]                     0       easy   \n",
       "3  [Option A, Option B, Option C, Option D]                     1     medium   \n",
       "\n",
       "                      topic    source_excerpt         source_doc_id  \n",
       "0  probability distribution  Dummy excerpt 1.  dummy:page=0:chunk=0  \n",
       "1  probability distribution  Dummy excerpt 2.  dummy:page=0:chunk=1  \n",
       "2  probability distribution  Dummy excerpt 1.  dummy:page=0:chunk=0  \n",
       "3  probability distribution  Dummy excerpt 2.  dummy:page=0:chunk=1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result_state[\"questions_mcq\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaa03f7-7243-4ae4-9870-3f906348f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MCQs for evaluation to: C:\\Users\\Admin\\OneDrive\\Desktop\\Capstone-MAT496\\data\\processed\\langgraph_mcqs_dummy.csv\n"
     ]
    }
   ],
   "source": [
    "questions_df = pd.DataFrame(result_state[\"questions_mcq\"])\n",
    "eval_input_path = PROCESSED_DIR / \"langgraph_mcqs_dummy.csv\"\n",
    "questions_df.to_csv(eval_input_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved MCQs for evaluation to:\", eval_input_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
